{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea1a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1\n",
    "rcv1 = fetch_rcv1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83ef751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import average_precision_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df80b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rcv1.data[:10000]\n",
    "y = rcv1.target[:10000]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d53ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = OneVsRestClassifier(\n",
    "    LogisticRegression(\n",
    "        solver=\"saga\",           # supports L1/L2 + probability\n",
    "        penalty=\"l2\",\n",
    "        C=1.0,\n",
    "        max_iter=200,\n",
    "        n_jobs=-1,              # ignored by base LR, parallel via OneVsRest\n",
    "        class_weight=\"balanced\" # good for imbalance\n",
    "    ),\n",
    "    n_jobs=-1\n",
    ")\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Probabilities for PR-AUC\n",
    "y_score_lr = logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae91d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrices to dense arrays\n",
    "y_test_dense = y_test.toarray() if hasattr(y_test, \"toarray\") else y_test\n",
    "y_score_lr_dense = y_score_lr.toarray() if hasattr(y_score_lr, \"toarray\") else y_score_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ecc73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR One-vs-Rest — PR-AUC (micro): 0.1376\n",
      "LR One-vs-Rest — F1 micro: 0.3064 | F1 macro: 0.2892\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.78      0.26        58\n",
      "           1       0.15      0.93      0.26        28\n",
      "           2       0.17      0.84      0.29        89\n",
      "           3       1.00      0.06      0.12        16\n",
      "           4       0.91      0.90      0.91       397\n",
      "           5       0.84      0.93      0.88       232\n",
      "           6       0.13      0.97      0.23        34\n",
      "           7       0.70      0.85      0.77       178\n",
      "           8       0.03      1.00      0.06         5\n",
      "           9       0.88      0.16      0.27        95\n",
      "          10       0.80      0.32      0.45        38\n",
      "          11       0.13      0.85      0.22        20\n",
      "          12       0.02      1.00      0.04         5\n",
      "          13       0.15      0.95      0.27        20\n",
      "          14       0.59      0.90      0.71       119\n",
      "          15       0.73      0.54      0.62        99\n",
      "          16       0.04      0.93      0.08        15\n",
      "          17       0.09      1.00      0.16        20\n",
      "          18       0.14      0.91      0.24        70\n",
      "          19       0.04      1.00      0.07         9\n",
      "          20       0.01      1.00      0.01         2\n",
      "          21       0.29      0.88      0.44        93\n",
      "          22       0.30      0.82      0.44       113\n",
      "          23       0.05      0.94      0.10        18\n",
      "          24       0.00      0.00      0.00        23\n",
      "          25       0.00      1.00      0.00         5\n",
      "          26       0.00      1.00      0.01         2\n",
      "          27       0.16      0.79      0.27        42\n",
      "          28       0.00      1.00      0.00         1\n",
      "          29       0.05      1.00      0.09         8\n",
      "          30       0.08      1.00      0.15        14\n",
      "          31       0.07      0.79      0.13        14\n",
      "          32       0.13      0.96      0.22        25\n",
      "          33       0.93      0.92      0.93       945\n",
      "          34       0.04      1.00      0.08        29\n",
      "          35       0.63      0.35      0.45        68\n",
      "          36       1.00      0.50      0.67        10\n",
      "          37       0.38      0.80      0.52        30\n",
      "          38       0.16      1.00      0.27        28\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.01      1.00      0.02         5\n",
      "          41       0.00      1.00      0.00         1\n",
      "          42       0.00      0.00      0.00         0\n",
      "          43       0.00      1.00      0.01         2\n",
      "          44       0.64      0.90      0.75       123\n",
      "          45       0.17      0.97      0.29        39\n",
      "          46       0.82      0.87      0.85        86\n",
      "          47       0.00      0.00      0.00         6\n",
      "          48       0.01      1.00      0.01         3\n",
      "          49       0.00      0.00      0.00         0\n",
      "          50       0.00      0.00      0.00         0\n",
      "          51       0.21      0.94      0.34        31\n",
      "          52       0.01      1.00      0.02         4\n",
      "          53       0.16      0.79      0.27        47\n",
      "          54       0.01      1.00      0.03         6\n",
      "          55       0.07      1.00      0.12        32\n",
      "          56       0.01      1.00      0.02         2\n",
      "          57       0.00      0.00      0.00         1\n",
      "          58       0.09      1.00      0.16        14\n",
      "          59       0.71      0.86      0.78       317\n",
      "          60       0.13      0.96      0.23        26\n",
      "          61       0.00      0.00      0.00         1\n",
      "          62       0.01      1.00      0.02         2\n",
      "          63       0.03      1.00      0.06         7\n",
      "          64       1.00      0.50      0.67         4\n",
      "          65       0.01      1.00      0.02         3\n",
      "          66       0.00      0.00      0.00         0\n",
      "          67       0.04      1.00      0.08         7\n",
      "          68       0.03      1.00      0.05         7\n",
      "          69       0.00      0.00      0.00         0\n",
      "          70       0.90      0.94      0.92       530\n",
      "          71       0.44      0.99      0.61        74\n",
      "          72       0.11      0.96      0.20        27\n",
      "          73       0.43      0.85      0.57        78\n",
      "          74       0.12      1.00      0.22        21\n",
      "          75       0.02      1.00      0.03         8\n",
      "          76       0.60      0.21      0.32        14\n",
      "          77       0.00      0.00      0.00         0\n",
      "          78       0.50      0.06      0.11        16\n",
      "          79       0.23      1.00      0.38        33\n",
      "          80       0.00      0.00      0.00         0\n",
      "          81       0.00      0.00      0.00         0\n",
      "          82       0.01      1.00      0.02         5\n",
      "          83       0.50      0.93      0.65       120\n",
      "          84       0.03      0.89      0.07         9\n",
      "          85       0.01      1.00      0.01         2\n",
      "          86       0.01      1.00      0.02         3\n",
      "          87       0.98      0.94      0.96        67\n",
      "          88       0.00      0.00      0.00         0\n",
      "          89       0.55      0.99      0.71        97\n",
      "          90       0.12      1.00      0.21        20\n",
      "          91       0.13      1.00      0.23        19\n",
      "          92       0.02      1.00      0.03         4\n",
      "          93       0.72      0.96      0.83       104\n",
      "          94       0.93      0.21      0.34        68\n",
      "          95       0.88      0.73      0.80       143\n",
      "          96       0.58      0.94      0.72        88\n",
      "          97       0.41      0.95      0.57        58\n",
      "          98       0.88      0.96      0.92       232\n",
      "          99       0.92      0.92      0.92       144\n",
      "         100       0.24      1.00      0.39        27\n",
      "         101       0.48      0.96      0.64        47\n",
      "         102       0.90      0.94      0.92       521\n",
      "\n",
      "   micro avg       0.19      0.87      0.31      6374\n",
      "   macro avg       0.28      0.75      0.29      6374\n",
      "weighted avg       0.69      0.87      0.71      6374\n",
      " samples avg       0.20      0.89      0.32      6374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, average_precision_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# PR-AUC (micro)\n",
    "pr_auc_micro_lr = average_precision_score(y_test_dense, y_score_lr_dense, average=\"micro\")\n",
    "\n",
    "# Threshold at 0.5 for F1\n",
    "y_pred_lr = (y_score_lr_dense >= 0.5).astype(int)\n",
    "\n",
    "f1_micro_lr = f1_score(y_test_dense, y_pred_lr, average=\"micro\", zero_division=0)\n",
    "f1_macro_lr = f1_score(y_test_dense, y_pred_lr, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(f\"LR One-vs-Rest — PR-AUC (micro): {pr_auc_micro_lr:.4f}\")\n",
    "print(f\"LR One-vs-Rest — F1 micro: {f1_micro_lr:.4f} | F1 macro: {f1_macro_lr:.4f}\")\n",
    "\n",
    "# (Optional) per-class report\n",
    "print(classification_report(y_test_dense, y_pred_lr, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687b2d5",
   "metadata": {},
   "source": [
    "## Logistice Regression Results Vs Neural Nets Results\n",
    "\n",
    "| Metric         | Logistic Regression         | Neural Network                                 |\n",
    "| -------------- | --------------------------- | ---------------------------------------------- |\n",
    "| PR-AUC (micro) | **0.14**                    | **~0.58**                                      |\n",
    "| F1 (micro)     | **0.31**                    | **~0.82**                                      |\n",
    "| F1 (macro)     | **0.29**                    | **~0.45**                                      |\n",
    "| Interpretation | Learns only linear patterns | Learns non-linear and contextual relationships |\n",
    "\n",
    "A One-vs-Rest Logistic Regression was used as a baseline for multi-label classification. While it achieved reasonable recall on common topics, its precision and PR-AUC were low (PR-AUC = 0.14, F1 micro = 0.31). In contrast, the neural network achieved significantly higher performance (PR-AUC ≈ 0.58, F1 micro ≈ 0.82), demonstrating that non-linear modeling captures more complex relationships between features and topic labels.\n",
    "\n",
    "**NOTE:** One-vs-Rest (OvR) — also called One-vs-All (OvA) — is a strategy used to extend Logistic Regression, which is naturally a binary classifier (i.e., predicts between two classes), so that it can handle multi-class or multi-label problems.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
